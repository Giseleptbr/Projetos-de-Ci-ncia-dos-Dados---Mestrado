# -*- coding: utf-8 -*-
"""exercicio 5 - tokenizaçao.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fAC_StMKXy6AyYPtHzCItrqaMFCBkxKj
"""

import nltk
nltk.download('punkt')  # baixar uma vez

from nltk.tokenize import sent_tokenize, word_tokenize

# Abra o arquivo no modo leitura
with open("untitled0.txt", "r", encoding='utf-8') as Arquivo:
    conteudo = Arquivo.read()  # Lê o conteúdo do arquivo como string

# Tokenizar sentenças
Sentencas_tokenizadas = sent_tokenize(conteudo)
print("Sentenças Tokenizadas:")
print(Sentencas_tokenizadas)

# Tokenizar palavras (convertendo o texto para minúsculas)
Palavras_tokenizadas = word_tokenize(conteudo.lower())
print("\nPalavras Tokenizadas:")
print(Palavras_tokenizadas)

from nltk.probability import FreqDist
Freq_Dist = FreqDist(Palavras_tokenizadas)
print(Freq_Dist, Freq_Dist.most_common(4))

import matplotlib.pyplot as plt
Freq_Dist.plot (30,cumulative=False)
plt.show ()

import nltk
from nltk.corpus import stopwords
from string import punctuation
nltk.download ('stopwords') # só será usado uma vez
Stop_Words = set (stopwords.words ("portuguese") +
list(punctuation))
print (Stop_Words)

Tokens_Limpos=[]
for Palavra in Palavras_tokenizadas:
    if Palavra not in Stop_Words:
        Tokens_Limpos.append(Palavra)

print ("Palavras tokenizadas (antes):", Palavras_tokenizadas)
print ("Palavras tokenizadas (depois):", Tokens_Limpos)

Freq_Dist_2 = FreqDist(Tokens_Limpos)
Freq_Dist_2.plot (30,cumulative=False)
plt.show ()

nltk.download('rslp') # ‘stemmizador’ (léxico) p/ português
Stemmer = nltk.stem.RSLPStemmer()
Palavras_Stemmed=[]
for Palavra in Tokens_Limpos:
 Palavras_Stemmed.append (Stemmer.stem (Palavra))
print("Palavras stemmed:", Palavras_Stemmed)

nltk.download ('averaged_perceptron_tagger') # uma vez
POS_Tag = nltk.pos_tag (Palavras_Stemmed)
print (POS_Tag)



